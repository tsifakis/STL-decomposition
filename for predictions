import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import joblib
import numpy as np
import pandas as pd
import tensorflow as tf
import statsmodels.api as sm
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics         import mean_squared_error, mean_absolute_error
from sklearn.preprocessing   import MinMaxScaler

from statsmodels.tsa.seasonal import STL

from tensorflow.keras.models       import Sequential
from tensorflow.keras.models       import Model, load_model
from tensorflow.keras.layers       import LSTM, Dropout, Dense, Input
from tensorflow.keras.callbacks    import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras              import backend as K

from colorama import Fore, Style
np.set_printoptions(precision=2, suppress=True)

#model = load_model('my_best_model4.keras')

seed_value = 1
tf.random.set_seed(seed_value)
np.random.seed(seed_value)

dataset_1 = np.array([0.04, 0.1, 0.05, 0.1, 0.07, 0.01, 0.09, 0.04, 0.05, 0.03, 0.05, 0.06, 0.09, 0.1, 0.08, 0.1, 0.09, 0.03, 0.06, 0.07, 0.02, 0.07, 0.06, 0.09, 0.1, 0.07, 0.08, 0.05, 0.09, 0.08, 0.01, 0.01, 0.08, 0.09, 0.06, 0.04, 0.06, 0.08, 0.03, 0.03, 0.08, 0.05, 0.09, 0.09, 0.07, 0.07, 0.1, 0.07, 0.1, 0.03, 0.08, 0.02, 0.04, 0.02, 0.08, 0.02, 0.01, 0.05, 0.05, 0.05, 0.09, 0.01, 0.07, 0.02, 0.1, 0.04, 0.08, 0.09, 0.06, 0.04, 0.03, 0.09, 0.08, 0.07, 0.04, 0.1, 0.02, 0.02, 0.01, 0.06, 0.08, 0.02, 0.08, 0.06, 0.09, 0.09, 0.03, 0.03, 0.07, 0.02, 0.04, 0.03, 0.01, 0.06, 0.1, 0.07, 0.08, 0.06, 0.05, 0.08, 0.04, 0.01, 0.1, 0.01, 0.05, 0.02, 0.08, 0.07, 0.03, 0.05, 0.08, 0.01, 0.06, 0.04, 0.1, 0.05, 0.1, 0.07, 0.06, 0.1, 0.01, 0.09, 0.06, 0.04, 0.06, 0.08, 0.06, 0.02, 0.05, 0.1, 0.09, 0.03, 0.03, 0.01, 0.03, 0.08, 0.1, 0.09, 0.02, 0.05, 0.09, 0.06, 0.03, 0.03, 0.06, 0.07, 0.04, 0.04, 0.05, 0.10, 0.10, 0.09, 0.08, 0.07, 0.09, 0.09, 0.06, 0.03, 0.02, 0.07, 0.09, 0.03, 0.04, 0.09 ])
dataset_2 = np.diff(dataset_1)
dataset_3 = np.array([0.08, 0.09, 0.05, 0.06, 0.10, 0.09, 0.02, 0.05, 0.02, 0.07, 0.05, 0.01, 0.10, 0.05, 0.07, 0.06, 0.02, 0.01, 0.02, 0.06, 0.06, 0.06, 0.02, 0.07, 0.07, 0.04, 0.07, 0.09, 0.04, 0.09, 0.02, 0.06, 0.05, 0.03, 0.10, 0.08, 0.05, 0.05, 0.05, 0.05, 0.04, 0.10, 0.01, 0.08, 0.01, 0.07, 0.09, 0.04, 0.05, 0.05, 0.07, 0.09, 0.07, 0.06, 0.09, 0.03, 0.10, 0.04, 0.01, 0.08, 0.07, 0.08, 0.02, 0.01, 0.08, 0.07, 0.04, 0.07, 0.06, 0.04, 0.06, 0.09, 0.04, 0.06, 0.03, 0.06, 0.07, 0.07, 0.07, 0.01, 0.08, 0.01, 0.02, 0.02, 0.10, 0.02, 0.09, 0.01, 0.05, 0.08, 0.02, 0.06, 0.02, 0.02, 0.07, 0.06, 0.07, 0.05, 0.03, 0.10, 0.10, 0.10, 0.03, 0.01, 0.03, 0.03, 0.10, 0.06, 0.01, 0.05, 0.06, 0.01, 0.05, 0.04, 0.08, 0.05, 0.09, 0.06, 0.09, 0.07, 0.09, 0.01, 0.07, 0.09, 0.01, 0.04, 0.05, 0.07, 0.09, 0.02, 0.05, 0.01, 0.05, 0.02, 0.06, 0.02, 0.08, 0.05, 0.10, 0.10, 0.10, 0.04, 0.06, 0.09, 0.10, 0.07, 0.01, 0.03, 0.02, 0.05, 0.05, 0.01, 0.01, 0.03, 0.04, 0.03, 0.08, 0.01, 0.08, 0.01, 0.04, ])
dataset_4 =  np.diff(dataset_3)

datasets = [dataset_1, dataset_2, dataset_3, dataset_4]
names = ["dataset_1", "dataset_2", "dataset_3", "dataset_4"]

def create_dataset(data, look_back=1):
    X, y = [], []
    for i in range(len(data) - look_back):
        a = data[i:(i + look_back), :]
        X.append(a)
        y.append(data[i + look_back, :])
    return np.array(X), np.array(y)

start_date = '2024-01-01'
datasets_with_date = []
for data in datasets:
    dates = pd.date_range(start=start_date, periods=len(data), freq='D')
    datasets_with_date.append(pd.Series(data, index=dates))

models = []
scalers = []

for i, data_series in enumerate(datasets_with_date):

    seed_value = 1
    tf.random.set_seed(seed_value)
    np.random.seed(seed_value)

    result = STL(data_series, seasonal=5, robust=True).fit()
    data_combined = np.column_stack((result.seasonal, result.trend, result.resid))
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(data_combined)

    X, y = create_dataset(scaled_data, look_back=1)
    split_point = int(len(X) * 0.8)
    X_train, X_test = X[:split_point], X[split_point:]
    y_train, y_test = y[:split_point], y[split_point:]

    look_back=1
    features = 3

    input_layer = Input(shape=(look_back, features))
    lstm1, state_h1, state_c1 = (LSTM(32,
                                      return_sequences=True,
                                      return_state=True,
                                      activation='tanh',
                                      recurrent_activation='sigmoid',
                                      use_bias=True,
                                      unit_forget_bias=True,
                                      go_backwards=False,
                                      stateful=False,
                                      unroll=True,
                                      recurrent_dropout=0.1, )
                                 (input_layer))
    dropout1 = Dropout(0.1)(lstm1)
    lstm2, state_h2, state_c2 = (LSTM(32,
                                      return_sequences=True,
                                      return_state=True,
                                      activation='tanh',
                                      recurrent_activation='sigmoid',
                                      use_bias=True,
                                      unit_forget_bias=True,
                                      go_backwards=False,
                                      stateful=False,
                                      unroll=True,
                                      recurrent_dropout=0.1, )
                                 (dropout1, initial_state=[state_h1, state_c1]))
    dropout2 = Dropout(0.1)(lstm2)
    lstm3, state_h3, state_c3 = (LSTM(32,
                                      return_sequences=False,
                                      return_state=True,
                                      activation='tanh',
                                      recurrent_activation='sigmoid',
                                      use_bias=True,
                                      unit_forget_bias=True,
                                      go_backwards=False,
                                      stateful=False,
                                      unroll=True,
                                      recurrent_dropout=0.1, )
                                 (dropout2, initial_state=[state_h2, state_c2]))
    dropout3 = Dropout(0.1)(lstm3)
    output_layer = (Dense(3,
                          activation='linear',
                          use_bias=True, )
                    (dropout3))
    model = Model(inputs=input_layer, outputs=output_layer)

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error',
                  metrics=[tf.keras.metrics.MeanSquaredError(),
                           tf.keras.metrics.MeanAbsoluteError()])

    early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=2, mode='min', restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=30, verbose=2, mode='min', factor=0.01, min_lr=0.0000001)
    history = model.fit(X_train, y_train, epochs=150, batch_size=1, verbose=2, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr])

    model.save(f'model_{names[i]}.keras')
    joblib.dump(scaler, f'scaler_{names[i]}.pkl')
    models.append(model)
    scalers.append(scaler)

    tf.keras.backend.clear_session()

all_final_prediction=[]
for i, model in enumerate(models):
    scaler = scalers[i]
    result = STL(datasets_with_date[i], seasonal=7, robust=True).fit()
    data_combined = np.column_stack((result.seasonal, result.trend, result.resid))

    last_data = data_combined[-1].reshape(1, -1)
    scaled_last_data = scaler.transform(last_data)
    scaled_last_data = scaled_last_data.reshape(1, 1, 3)

    prediction = model.predict(scaled_last_data)
    prediction_rescaled = scaler.inverse_transform(prediction)

    final_prediction = sum(prediction_rescaled[0])
    print(f"Προβλεπόμενη επόμενη τιμή για το {names[i]}: {final_prediction:.2f}")
    all_final_prediction.append(final_prediction)

    print(f"{Fore.BLUE}\nΙστορικό εκπαίδευσης:{Style.RESET_ALL}")
    for key in history.history.keys():
        print(f"{Fore.RED}{key}:{Style.RESET_ALL} {history.history[key][-1]:.2f}")
    print()

predicted_1 = all_final_prediction[-4]
predicted_1 = np.array([predicted_1])  # Make it an array
predicted_1 = np.clip(predicted_1, 0.01, 0.1)

predicted_2 = all_final_prediction[-3]
difference2 = dataset_1[-1] + predicted_2
difference2 = np.array([difference2])  # Make it an array
difference2 = np.clip(difference2, 0.01, 0.1)

predicted_3 = all_final_prediction[-2]
predicted_3 = np.array([predicted_3])  # Make it an array
predicted_3 = np.clip(predicted_3, 0.01, 0.1)

predicted_4 = all_final_prediction[-1]
difference4 = dataset_3[-1] + predicted_4
difference4 = np.array([difference4])  # Make it an array
difference4 = np.clip(difference4, 0.01, 0.1)

std_dev_1 = np.random.uniform(0.01, 0.05)
random_noise_1 = np.random.normal(0.05, std_dev_1, size=predicted_1.shape)
noise_sign1 = np.random.choice([-1, 1], size=predicted_1.shape)
predicted_1_with_noise_1 = predicted_1 + (random_noise_1 * noise_sign1)
predicted_1_with_noise_1 = np.clip(predicted_1_with_noise_1, 0.01, 0.1)

std_dev_2 = np.random.uniform(0.01, 0.05)
random_noise_2 = np.random.normal(0.05, std_dev_1, size=predicted_3.shape)
noise_sign2 = np.random.choice([-1, 1], size=predicted_3.shape)
predicted_3_with_noise_1 = predicted_3 + (random_noise_2 * noise_sign2)
predicted_3_with_noise_1 = np.clip(predicted_3_with_noise_1, 0.01, 0.1)

predicted_values = [
    round(predicted_1[0] * 100),
    round(difference2[0] * 100),
    round(predicted_3[0] * 100),
    round(difference4[0] * 100),
    round(predicted_3_with_noise_1[0] * 100),
    round(predicted_1_with_noise_1[0] * 100)]

unique_sorted_values = sorted(set(predicted_values))

for index, predicted in enumerate(all_final_prediction):
    print(f"{Fore.CYAN}Προβλέψεις για το dataset {index + 1}: {Style.RESET_ALL}{predicted:.2f}")

print(f"\n{Fore.MAGENTA}Προβλέψεις καθαρές για το dataset 1: {Style.RESET_ALL}{predicted_1[0]:.2f}")
print(f"{Fore.MAGENTA}Προβλέψεις με διαφορά για το dataset 1: {Style.RESET_ALL}{difference2[0]:.2f}")
print(f"{Fore.MAGENTA}Προβλέψεις με τυχαιότητα για dataset 1: {Style.RESET_ALL}{predicted_1_with_noise_1[0]:.2f}")

print(f"\n{Fore.MAGENTA}Προβλέψεις καθαρές για το dataset 3: {Style.RESET_ALL}{predicted_3[0]:.2f}")
print(f"{Fore.MAGENTA}Προβλέψεις με διαφορά για το dataset 3: {Style.RESET_ALL}{difference4[0]:.2f}")
print(f"{Fore.MAGENTA}Προβλέψεις με τυχαιότητα για dataset 3: {Style.RESET_ALL}{predicted_3_with_noise_1[0]:.2f}")

print(f"\n{Fore.GREEN}ΟΛΕΣ ΟΙ ΠΡΟΒΛΕΨΕΙΣ: {Style.RESET_ALL} {[round(predicted_1[0]*100), round(difference2[0]*100),  round(predicted_1_with_noise_1[0]*100),  round(predicted_3[0]*100), round(difference4[0]*100), round(predicted_3_with_noise_1[0]*100)]}")
print(f"{Fore.RED}ΤΕΛΕΥΤΑΙΑ ΤΙΜΗ ΑΠΟ ΚΑΘΕ dataset:{Style.RESET_ALL}", dataset_1[-1]*100, dataset_2[-1]*100, dataset_3[-1]*100, dataset_4[-1]*100  )
print()
print()
print(f"\n{Fore.BLUE}ΟΛΕΣ ΟΙ ΕΠΟΜΕΝΕΣ ΘΕΡΜΟΚΡΑΣΙΕΣ: {Style.RESET_ALL}{Fore.YELLOW}{unique_sorted_values}{Style.RESET_ALL}")
print()




